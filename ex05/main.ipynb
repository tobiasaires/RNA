{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos um modelo GRU para uma tarefa de previsão de série de tempo. O conjunto de dados que usaremos é o conjunto de dados de consumo de energia por hora. O conjunto de dados contém dados de consumo de energia em diferentes regiões dos Estados Unidos registrados de hora em hora.\n",
    "\n",
    "Estaremos usando a biblioteca PyTorch para implementar o tipo de modelo GRU junto com outras bibliotecas Python comuns usadas na análise de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Obs: Por motivos de baixos recursos computacionais e tempo, simplifiquei os dados, mas no arquivo data/completo_AEP_hourly.csv você encontra os dados completos<h3/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pjm_hourly_est.csv', 'AEP_hourly.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import time\n",
    "from time import process_time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Definindo o diretório raiz de dados\n",
    "data_dir = \"./data/\"\n",
    "print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>AEP_MW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004-12-31 01:00:00</td>\n",
       "      <td>13478.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004-12-31 02:00:00</td>\n",
       "      <td>12865.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004-12-31 03:00:00</td>\n",
       "      <td>12577.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004-12-31 04:00:00</td>\n",
       "      <td>12517.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004-12-31 05:00:00</td>\n",
       "      <td>12670.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Datetime   AEP_MW\n",
       "0  2004-12-31 01:00:00  13478.0\n",
       "1  2004-12-31 02:00:00  12865.0\n",
       "2  2004-12-31 03:00:00  12577.0\n",
       "3  2004-12-31 04:00:00  12517.0\n",
       "4  2004-12-31 05:00:00  12670.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(data_dir+'AEP_hourly.csv').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos um total de 2 arquivos .csv contendo dados de tendência de energia por hora ('pjm_hourly_est.csv' não é usado). Em nossa próxima etapa, leremos esses arquivos e pré-processaremos esses dados nesta ordem:\n",
    "\n",
    "- Obter os dados de cada etapa de tempo individual e generalizá-los\n",
    "    - Hour of the day *i.e. 0-23*\n",
    "    - Day of the week *i.e. 1-7*\n",
    "    - Month *i.e. 1-12*\n",
    "    - Day of the year *i.e. 1-365*\n",
    "    \n",
    "    \n",
    "- Dimensionando os dados para valores entre 0 e 1\n",
    "    - Algoritmos tendem a ter um desempenho melhor ou convergir mais rápido quando os recursos estão em uma escala relativamente semelhante e / ou perto da distribuição normal\n",
    "     - O dimensionamento preserva a forma da distribuição original e não reduz a importância.\n",
    "    \n",
    "    \n",
    "- Agrupamos os dados em sequências para serem usados como entradas para o modelo e armazenamos seus rótulos correspondentes\n",
    "   \n",
    "\n",
    "- As entradas e rótulos serão divididos em conjuntos de treinamento e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5504ce29cc84037bcaaa7383bcd22e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Os objetos agrupadores serão armazenados neste dicionário para que nossos dados de teste de saída do modelo possam ser redimensionados durante a avaliação\n",
    "label_scalers = {}\n",
    "\n",
    "train_x = []\n",
    "test_x = {}\n",
    "test_y = {}\n",
    "\n",
    "for file in tqdm_notebook(os.listdir(data_dir)): \n",
    "    # Ignorando os arquivos que não estamos usando\n",
    "    if file[-4:] != \".csv\" or file == \"pjm_hourly_est.csv\" or file == \"completo_AEP_hourly.csv\":\n",
    "        continue\n",
    "    \n",
    "    # Armazene o arquivo csv em um Pandas DataFrame\n",
    "    df = pd.read_csv(data_dir + file, parse_dates=[0])\n",
    "    # Processando os dados de tempo em formatos de entrada adequados\n",
    "    df['hour'] = df.apply(lambda x: x['Datetime'].hour,axis=1)\n",
    "    df['dayofweek'] = df.apply(lambda x: x['Datetime'].dayofweek,axis=1)\n",
    "    df['month'] = df.apply(lambda x: x['Datetime'].month,axis=1)\n",
    "    df['dayofyear'] = df.apply(lambda x: x['Datetime'].dayofyear,axis=1)\n",
    "    df = df.sort_values(\"Datetime\").drop(\"Datetime\",axis=1)\n",
    "    \n",
    "    # Escalonando os dados de entrada\n",
    "    sc = MinMaxScaler()\n",
    "    label_sc = MinMaxScaler()\n",
    "    data = sc.fit_transform(df.values)\n",
    "    # Obtendo a escala para os rótulos (dados de uso) para que a saída possa ser redimensionada para o valor real durante a avaliação\n",
    "    label_sc.fit(df.iloc[:,0].values.reshape(-1,1))\n",
    "    label_scalers[file] = label_sc\n",
    "    \n",
    "    # Definindo o período de lookback e divida entradas / rótulos\n",
    "    lookback = 90\n",
    "    inputs = np.zeros((len(data)-lookback,lookback,df.shape[1]))\n",
    "    labels = np.zeros(len(data)-lookback)\n",
    "    \n",
    "    for i in range(lookback, len(data)):\n",
    "        inputs[i-lookback] = data[i-lookback:i]\n",
    "        labels[i-lookback] = data[i,0]\n",
    "    inputs = inputs.reshape(-1,lookback,df.shape[1])\n",
    "    labels = labels.reshape(-1,1)\n",
    "    \n",
    "    # Dividindo os dados em porções de treinamento / teste e combinando todos os dados\n",
    "    test_portion = int(0.1*len(inputs))\n",
    "    if len(train_x) == 0:\n",
    "        train_x = inputs[:-test_portion]\n",
    "        train_y = labels[:-test_portion]\n",
    "    else:\n",
    "        train_x = np.concatenate((train_x,inputs[:-test_portion]))\n",
    "        train_y = np.concatenate((train_y,labels[:-test_portion]))\n",
    "    test_x[file] = (inputs[-test_portion:])\n",
    "    test_y[file] = (labels[-test_portion:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para melhorar a velocidade de nosso treinamento, podemos processar os dados em lotes para que o modelo não precise atualizar seus pesos com tanta frequência. As classes Torch * Dataset * e * DataLoader * são úteis para dividir nossos dados em lotes e embaralhá-los."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_cuda = torch.cuda.is_available()\n",
    "# if is_cuda:\n",
    "#     device = torch.device(\"cuda\")\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir, definiremos a estrutura do modelo GRU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, learn_rate, hidden_dim=256, EPOCHS=5, model_type=\"GRU\"):\n",
    "    \n",
    "    input_dim = next(iter(train_loader))[0].shape[2]\n",
    "    output_dim = 1\n",
    "    n_layers = 2\n",
    "    # Instanciando o modelo\n",
    "    model = GRUNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    \n",
    "    # Definição de função de perda e otimizador\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "    \n",
    "    model.train()\n",
    "    print(\"Starting Training of {} model\".format(model_type))\n",
    "    epoch_times = []\n",
    "    # Iniciando loop de treinamento\n",
    "    for epoch in range(1,EPOCHS+1):\n",
    "        start_time = process_time()\n",
    "        h = model.init_hidden(batch_size)\n",
    "        avg_loss = 0.\n",
    "        counter = 0\n",
    "        for x, label in train_loader:\n",
    "            counter += 1\n",
    "            h = h.data\n",
    "            \n",
    "            out, h = model(x.to(device).float(), h)\n",
    "            loss = criterion(out, label.to(device).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item()\n",
    "            if counter%200 == 0:\n",
    "                print(\"Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}\".format(epoch, counter, len(train_loader), avg_loss/counter))\n",
    "        current_time = time.process_time()\n",
    "        print(\"Epoch {}/{} Done, Total Loss: {}\".format(epoch, EPOCHS, avg_loss/len(train_loader)))\n",
    "        print(\"Time Elapsed for Epoch: {} seconds\".format(str(current_time-start_time)))\n",
    "        epoch_times.append(current_time-start_time)\n",
    "    print(\"Total Training Time: {} seconds\".format(str(sum(epoch_times))))\n",
    "    return model\n",
    "\n",
    "def evaluate(model, test_x, test_y, label_scalers):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    start_time = process_time()\n",
    "    for i in test_x.keys():\n",
    "        inp = torch.from_numpy(np.array(test_x[i]))\n",
    "        labs = torch.from_numpy(np.array(test_y[i]))\n",
    "        h = model.init_hidden(inp.shape[0])\n",
    "        out, h = model(inp.to(device).float(), h)\n",
    "        outputs.append(label_scalers[i].inverse_transform(out.cpu().detach().numpy()).reshape(-1))\n",
    "        targets.append(label_scalers[i].inverse_transform(labs.numpy()).reshape(-1))\n",
    "    print(\"Evaluation Time: {}\".format(str(time.process_time()-start_time)))\n",
    "    sMAPE = 0\n",
    "    for i in range(len(outputs)):\n",
    "        sMAPE += np.mean(abs(outputs[i]-targets[i])/(targets[i]+outputs[i])/2)/len(outputs)\n",
    "    print(\"sMAPE: {}%\".format(sMAPE*100))\n",
    "    return outputs, targets, sMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training of GRU model\n",
      "Epoch 1/5 Done, Total Loss: 0.1798754185438156\n",
      "Time Elapsed for Epoch: 37.031527326 seconds\n",
      "Epoch 2/5 Done, Total Loss: 0.0985664427280426\n",
      "Time Elapsed for Epoch: 43.849040951999996 seconds\n",
      "Epoch 3/5 Done, Total Loss: 0.04450027644634247\n",
      "Time Elapsed for Epoch: 43.026285185999996 seconds\n",
      "Epoch 4/5 Done, Total Loss: 0.026159686967730522\n",
      "Time Elapsed for Epoch: 53.944850213999985 seconds\n",
      "Epoch 5/5 Done, Total Loss: 0.06714832037687302\n",
      "Time Elapsed for Epoch: 39.53589641100007 seconds\n",
      "Total Training Time: 217.38760008900005 seconds\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "gru_model = train(train_loader, lr, model_type=\"GRU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Time: 1.8103908580000052\n",
      "sMAPE: 4.945344692085785%\n"
     ]
    }
   ],
   "source": [
    "gru_outputs, targets, gru_sMAPE = evaluate(gru_model, test_x, test_y, label_scalers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
